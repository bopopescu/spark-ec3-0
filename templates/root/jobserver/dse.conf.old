spark {
  master = "spark://sparkmaster.qion.agr.br:7077"

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 4
  jobserver {
    port = 8090
    jobdao = spark.jobserver.io.JobSqlDAO
    max-jobs-per-context = 24
    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.driver.H2Driver

      # JDBC driver, full classpath
      jdbc-driver = org.h2.Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = "/var/spark-jobserver/sqldao/data"

      jdbc {
        url = "jdbc:h2:file:/var/spark-jobserver/sqldao/data/h2-db"
        user = "secret"
        password = "secret"
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
  }

  # predefined Spark contexts
  contexts {
    cass-context {
      memory-per-node = 8gb
      num-cpu-cores = 4	
      spark.cassandra.connection.host = 54.233.144.172
      context-factory = spark.jobserver.context.SQLContextFactory
      spark.serialize = org.apache.spark.serializer.KryoSerializer
      spark.executor.extraClassPath = /usr/spark/jdbc/postgres.jar
      spark.metrics.conf = /usr/spark/conf/metrics.properties
    }
  }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 4          # Number of cores to allocate.  Required.
    memory-per-node = 8gb         # Executor memory per node, -Xmx style eg 512m, #1G, etc.
  }
}
