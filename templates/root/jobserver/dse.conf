spark {
  master = "spark://sparkmaster.qion.agr.br:7077"

  # Default # of CPUs for jobs to use for Spark standalone cluster
  job-number-cpus = 8
  jobserver {
    port = 8090
#    context-per-jvm = true
    jobdao = spark.jobserver.io.JobSqlDAO
    max-jobs-per-context = 24

    sqldao {
      # Slick database driver, full classpath
      slick-driver = slick.driver.H2Driver

      # Directory where default H2 driver stores its data. Only needed for H2.
      rootdir = /var/spark-jobserver/sqldao/data

      # JDBC driver, full classpath
      jdbc-driver = org.h2.Driver

      jdbc {
        url = "jdbc:h2:file:/var/spark-jobserver/sqldao/data/h2-db"
        user = "secret"
        password = "secret"
      }

      dbcp {
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
  }

  # predefined Spark contexts
  contexts {
    cass-context {
      memory-per-node = 12gb
      num-cpu-cores = 8
      spark.cassandra.connection.host = cassandra.qion.agr.br
      context-factory = spark.jobserver.context.SQLContextFactory
      spark.serialize = org.apache.spark.serializer.KryoSerializer
      spark.executor.extraClassPath = /usr/spark/jdbc/postgres.jar
      spark.metrics.conf = /usr/spark/conf/metrics.properties
    }
  }

  # universal context configuration.  These settings can be overridden, see README.md
  context-settings {
    num-cpu-cores = 8          		# Number of cores to allocate.  Required.
    memory-per-node = 12gb        	# Executor memory per node, -Xmx style eg 512m, #1G, etc.
  }
 home = "/usr/spark"
}
