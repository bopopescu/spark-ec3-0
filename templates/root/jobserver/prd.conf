# Spark Cluster / Job Server configuration
spark {
  master = ${SPARK_MASTER}
  submit.deployMode = "cluster"
  job-number-cpus = 4

  jobserver {
    port = 8090
    context-per-jvm = true
    jobdao = spark.jobserver.io.JobSqlDAO
    max-jobs-per-context = 48

    filedao {
      rootdir = /mnt/spark-jobserver/filedao/data
    }

    datadao {
      rootdir = /mnt/spark-jobserver/upload
    }

    sqldao {
      slick-driver = slick.driver.H2Driver
      jdbc-driver = org.h2.Driver
      rootdir = /mnt/spark-jobserver/sqldao/data

      jdbc {
        url = ${H2_JDBC}
      }

      dbcp {
        enabled = false
        maxactive = 20
        maxidle = 10
        initialsize = 10
      }
    }
    result-chunk-size = 1m
  }

  contexts {
    cass-context {
      memory-per-node = 20gb
      num-cpu-cores = 16
      spark.cassandra.connection.host = ${CASSANDRA_HOST}
      context-factory = spark.jobserver.context.SessionContextFactory
      spark.serialize = org.apache.spark.serializer.KryoSerializer
      spark.executor.extraClassPath = /root/conf/jdbc/postgres.jar
      spark.scheduler.mode = FAIR
      spark.scheduler.pool = production
    }
  }

  context-settings {
    num-cpu-cores = 2
    memory-per-node = 512m
  }

  home="/root/spark"
}

# start akka on this interface, reachable from your cluster
akka {
  remote.netty.tcp {
    hostname = ${MASTER_HOST}

    # This controls the maximum message size, including job results, that can be sent
    maximum-frame-size = 100 MiB
  }
}